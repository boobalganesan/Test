import faiss
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory

# Load embedding model (FinBERT)
embedding_model = SentenceTransformer("ProsusAI/finbert")

# Load LLaMA model (Replace with your own model)
llama_model = "your_llama_finetuned_model"  
llama_pipeline = pipeline("text-generation", model=llama_model, max_length=512)

# Load and process documents
loader = TextLoader("finance_docs.txt")  # Replace with your finance text file
documents = loader.load()

# Split text into chunks
text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# Create FAISS index
faiss_index = FAISS.from_documents(docs, HuggingFaceEmbeddings(model_name="ProsusAI/finbert"))

# Setup memory for chat history
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Create the RAG QA Chain
qa_chain = RetrievalQA.from_chain_type(llm=llama_pipeline, retriever=faiss_index.as_retriever())

# Chat Loop
print("Chatbot is ready! Type 'exit' to stop.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        print("Goodbye!")
        break
    
    response = qa_chain.run(user_input)
    print("Bot:", response)
